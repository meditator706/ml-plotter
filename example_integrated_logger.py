"""
ML Plotter Logger é›†æˆä½¿ç”¨ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•ä½¿ç”¨é›†æˆçš„å®éªŒæ—¥å¿—è®°å½•å’Œå¯è§†åŒ–åŠŸèƒ½
"""

import numpy as np
import time
from ml_plotter_logger import MLPlotterLogger, create_integrated_logger, quick_experiment_plot

def simulate_training_run(config: dict, steps: int = 1000) -> dict:
    """æ¨¡æ‹Ÿä¸€ä¸ªè®­ç»ƒè¿‡ç¨‹"""
    results = {'steps': [], 'episodic_return': [], 'actor_loss': [], 'critic_loss': []}
    
    # æ ¹æ®é…ç½®è®¾ç½®åŸºç¡€æ€§èƒ½
    base_reward = 100
    if config.get('algorithm') == 'TD3':
        base_reward += 50
    elif config.get('algorithm') == 'SAC':
        base_reward += 30
    
    if config.get('lr') == 0.001:
        base_reward += 20
    elif config.get('lr') == 0.0001:
        base_reward -= 10
    
    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
    for step in range(0, steps, 50):
        # å¥–åŠ±éšæ—¶é—´å¢é•¿ï¼Œä½†æœ‰å™ªå£°
        reward = base_reward + step * 0.1 + np.random.normal(0, 10)
        
        # æŸå¤±éšæ—¶é—´å‡å°‘
        actor_loss = 2.0 * np.exp(-step/300) + np.random.normal(0, 0.1)
        critic_loss = 5.0 * np.exp(-step/200) + np.random.normal(0, 0.2)
        
        results['steps'].append(step)
        results['episodic_return'].append(max(0, reward))  # ç¡®ä¿éè´Ÿ
        results['actor_loss'].append(max(0, actor_loss))
        results['critic_loss'].append(max(0, critic_loss))
    
    return results

def example_1_basic_logging():
    """ç¤ºä¾‹1: åŸºæœ¬çš„æ—¥å¿—è®°å½•å’Œå¯è§†åŒ–"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹1: åŸºæœ¬çš„æ—¥å¿—è®°å½•å’Œå¯è§†åŒ–")
    print("="*60)
    
    # åˆ›å»ºé›†æˆæ—¥å¿—è®°å½•å™¨
    logger = create_integrated_logger("demo_logs")
    
    # é…ç½®å®éªŒ
    experiment_id = "basic_rl_experiment"
    configs = [
        {'algorithm': 'TD3', 'lr': 0.001, 'seed': 1},
        {'algorithm': 'TD3', 'lr': 0.001, 'seed': 2},
        {'algorithm': 'SAC', 'lr': 0.001, 'seed': 1},
        {'algorithm': 'SAC', 'lr': 0.001, 'seed': 2},
    ]
    
    # è¿è¡Œå®éªŒ
    for i, config in enumerate(configs):
        run_id = f"run_{i}"
        
        # å¼€å§‹è¿è¡Œ
        logger.start_run(experiment_id, run_id, config, 
                        tags=[f"algo:{config['algorithm']}", f"lr:{config['lr']}"])
        
        # æ¨¡æ‹Ÿè®­ç»ƒ
        results = simulate_training_run(config)
        
        # è®°å½•æ•°æ®
        for j, step in enumerate(results['steps']):
            logger.log(run_id, step, {
                'episodic_return': results['episodic_return'][j],
                'actor_loss': results['actor_loss'][j],
                'critic_loss': results['critic_loss'][j]
            })
        
        # ç»“æŸè¿è¡Œ
        logger.finish_run(run_id)
    
    # ç”Ÿæˆå¯è§†åŒ–
    print("\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    
    # æŒ‰ç®—æ³•åˆ†ç»„ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    fig1 = logger.plot_training_curves(
        experiment_id, 'episodic_return', 
        group_by='algorithm',
        title="Training Performance by Algorithm"
    )
    
    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    fig2 = logger.plot_training_curves(
        experiment_id, 'actor_loss',
        group_by='algorithm', 
        title="Actor Loss by Algorithm"
    )
    
    # ç»˜åˆ¶æ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾
    fig3 = logger.plot_comparison_bars(
        experiment_id, 'episodic_return',
        group_by='algorithm',
        title="Final Performance Comparison"
    )
    
    print("âœ… åŸºæœ¬ç¤ºä¾‹å®Œæˆï¼")

def example_2_auto_plotting():
    """ç¤ºä¾‹2: è‡ªåŠ¨ç»˜å›¾åŠŸèƒ½"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹2: è‡ªåŠ¨ç»˜å›¾åŠŸèƒ½")
    print("="*60)
    
    logger = create_integrated_logger("demo_logs")
    
    # é…ç½®æ›´å¤æ‚çš„å®éªŒ
    experiment_id = "hyperparameter_sweep"
    configs = [
        {'algorithm': 'TD3', 'lr': 0.001, 'batch_size': 256, 'seed': 1},
        {'algorithm': 'TD3', 'lr': 0.001, 'batch_size': 256, 'seed': 2},
        {'algorithm': 'TD3', 'lr': 0.0001, 'batch_size': 256, 'seed': 1},
        {'algorithm': 'TD3', 'lr': 0.0001, 'batch_size': 256, 'seed': 2},
        {'algorithm': 'TD3', 'lr': 0.001, 'batch_size': 128, 'seed': 1},
        {'algorithm': 'TD3', 'lr': 0.001, 'batch_size': 128, 'seed': 2},
    ]
    
    # è¿è¡Œå®éªŒ
    for i, config in enumerate(configs):
        run_id = f"sweep_run_{i}"
        
        logger.start_run(experiment_id, run_id, config)
        
        # æ¨¡æ‹Ÿè®­ç»ƒ
        results = simulate_training_run(config, steps=800)
        
        # è®°å½•æ•°æ®
        for j, step in enumerate(results['steps']):
            logger.log(run_id, step, {
                'episodic_return': results['episodic_return'][j],
                'actor_loss': results['actor_loss'][j],
                'critic_loss': results['critic_loss'][j]
            })
        
        logger.finish_run(run_id)
    
    # è‡ªåŠ¨ç”Ÿæˆæ‰€æœ‰å›¾è¡¨
    print("\nğŸ¨ è‡ªåŠ¨ç”Ÿæˆæ‰€æœ‰å›¾è¡¨...")
    figures = logger.auto_plot_experiment(
        experiment_id,
        group_by='lr',  # æŒ‰å­¦ä¹ ç‡åˆ†ç»„
        plot_types=['curves', 'bars', 'summary']
    )
    
    print(f"âœ… è‡ªåŠ¨ç”Ÿæˆäº† {len(figures)} ä¸ªå›¾è¡¨")

def example_3_experiment_comparison():
    """ç¤ºä¾‹3: å®éªŒå¯¹æ¯”"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹3: å®éªŒå¯¹æ¯”")
    print("="*60)
    
    logger = create_integrated_logger("demo_logs")
    
    # åˆ›å»ºä¸¤ä¸ªä¸åŒçš„å®éªŒ
    experiments = {
        'td3_experiment': [
            {'algorithm': 'TD3', 'lr': 0.001, 'seed': 1},
            {'algorithm': 'TD3', 'lr': 0.001, 'seed': 2},
            {'algorithm': 'TD3', 'lr': 0.001, 'seed': 3},
        ],
        'sac_experiment': [
            {'algorithm': 'SAC', 'lr': 0.001, 'seed': 1},
            {'algorithm': 'SAC', 'lr': 0.001, 'seed': 2},
            {'algorithm': 'SAC', 'lr': 0.001, 'seed': 3},
        ]
    }
    
    # è¿è¡Œä¸¤ä¸ªå®éªŒ
    for exp_id, configs in experiments.items():
        print(f"\nğŸ§ª è¿è¡Œå®éªŒ: {exp_id}")
        
        for i, config in enumerate(configs):
            run_id = f"{exp_id}_run_{i}"
            
            logger.start_run(exp_id, run_id, config)
            
            # æ¨¡æ‹Ÿè®­ç»ƒ
            results = simulate_training_run(config)
            
            # è®°å½•æ•°æ®
            for j, step in enumerate(results['steps']):
                logger.log(run_id, step, {
                    'episodic_return': results['episodic_return'][j],
                    'actor_loss': results['actor_loss'][j]
                })
            
            logger.finish_run(run_id)
    
    # æ¯”è¾ƒä¸¤ä¸ªå®éªŒ
    print("\nğŸ”„ æ¯”è¾ƒå®éªŒ...")
    fig_comparison = logger.compare_experiments(
        ['td3_experiment', 'sac_experiment'],
        'episodic_return',
        title="TD3 vs SAC Performance Comparison"
    )
    
    print("âœ… å®éªŒå¯¹æ¯”å®Œæˆï¼")

def example_4_quick_plot():
    """ç¤ºä¾‹4: å¿«é€Ÿç»˜å›¾"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹4: å¿«é€Ÿç»˜å›¾åŠŸèƒ½")
    print("="*60)
    
    # ä½¿ç”¨ä¾¿æ·å‡½æ•°å¿«é€Ÿç”Ÿæˆå›¾è¡¨
    figures = quick_experiment_plot(
        'basic_rl_experiment',  # ä½¿ç”¨ç¤ºä¾‹1çš„å®éªŒ
        log_dir="demo_logs",
        group_by='algorithm',
        plot_types=['curves', 'bars']
    )
    
    print(f"âœ… å¿«é€Ÿç”Ÿæˆäº† {len(figures)} ä¸ªå›¾è¡¨")

def main():
    """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("ğŸ¨ ML Plotter Logger é›†æˆç¤ºä¾‹")
    print("ç»“åˆå®éªŒæ—¥å¿—è®°å½•å’Œä¸“ä¸šå¯è§†åŒ–")
    
    try:
        # è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
        example_1_basic_logging()
        example_2_auto_plotting()
        example_3_experiment_comparison()
        example_4_quick_plot()
        
        print("\n" + "="*60)
        print("ğŸ‰ æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
        print("ğŸ“ æŸ¥çœ‹ç”Ÿæˆçš„æ–‡ä»¶:")
        print("   - æ—¥å¿—æ•°æ®: demo_logs/experiments.db")
        print("   - å›¾è¡¨æ–‡ä»¶: demo_logs/plots/")
        print("   - æ‘˜è¦æŠ¥å‘Š: demo_logs/plots/*_summary.md")
        print("="*60)
        
    except Exception as e:
        print(f"âŒ è¿è¡Œç¤ºä¾‹æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()